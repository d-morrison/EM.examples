---
title: "McLachlan-p-17"
output: 
  rmarkdown::html_vignette:
    code_folding: show
vignette: >
  %\VignetteIndexEntry{McLachlan-p-17}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup}
library(EM.examples)
library(ggplot2)
library(dplyr)
library(pander)
```

Example from McLachlan & Krishnan:

```{r}
rm(list = ls())
n = 50
mu = c(0,2)
sigma = 1
pi = c(.8,.2)
set.seed(1)
Z = rbinom(n = n, size = 1, p = pi[2]) |> factor(labels = c("Z = 0", "Z = 1"))
Y = rnorm(n = n, mean = ifelse(Z == "Z = 0", mu[1], mu[2]), sd = sigma)
hist(Y, breaks = 10)
data1 = tibble(Z, Y)

```

```{julia}
n = 50
mu = [0, 2]
sigma = 1
pi = [0.8, 0.2]
Random.seed!(1)
Z = rand(Bernoulli(pi[2]), n)
Z = map(x -> x ? "Z = 1" : "Z = 0", Z)
Y = randn(n) .* sigma .+ map(x -> x == "Z = 0" ? mu[1] : mu[2], Z)
hist(Y, bins=10)
data1 = DataFrame(Z = Z, Y = Y)
```

Here's the observed data:

```{r}
ggplot(data1, aes(x = Y)) +
  geom_histogram() +
  theme(axis.text=element_text(size=14),
    axis.title=element_text(size=18,face="bold"))

```

And here's the (latent) complete data:

```{r}
ggplot(data1, aes(x = Y)) +
  geom_histogram() +
  facet_wrap(~Z, ncol = 1) +
  theme(
    strip.text.x = element_text(size = 18),
    axis.text=element_text(size=14),
    axis.title=element_text(size=18,face="bold"))

```

```{r,fig.height=6}

`p(Y=y|Z=0)` = dnorm(Y, mu[1], sd = sigma)
`p(Y=y|Z=1)` = dnorm(Y, mu[2], sd = sigma)

likelihood = function(pi1)
{
  sapply(pi1, 
    function(x)
    {
      (x * `p(Y=y|Z=0)` + (1 - x) * `p(Y=y|Z=1)`) |> prod()
    })
  
}

loglik = function(pi1) log(likelihood(pi1))

par(mfrow = c(2,1))
plot(likelihood, xlim = c(0,1), xlab = "pi_1")
plot(loglik, xlim = c(0,1), xlab = "pi_1", ylab = "log(likelihood)")
```

# EM Algorithm

```{r, fig.height = 6, fig.width = 6}

`p(Z=0)`  = .5 # initial guess for `pi-hat`
diff = Inf
tolerance = .00001
progress = tibble(
  Iteration = 0,
  `p(Z=0)` = `p(Z=0)`,
  loglik =  loglik(`p(Z=0)`)
)
max_iterations = 1000

par(mfrow = c(1,1))

plot(loglik, xlim = c(0,1), xlab = "p(Z=0)", ylab = "log(likelihood)", lwd = 2)

for(i in 1:max_iterations)
{
  # E step:
  Estep = tibble(
    Y, # observed data
    `p(Y=y|Z=0)`,
    `p(Y=y|Z=1)`,
    `p(Y=y,Z=0)` = `p(Y=y|Z=0)`*`p(Z=0)`, # `p(Z=0)` = "pi-hat"
    `p(Y=y,Z=1)` = `p(Y=y|Z=1)`*(1 - `p(Z=0)`),
    `p(Y=y)` = `p(Y=y,Z=0)` + `p(Y=y,Z=1)`,
    `p(Z=0|Y=y)` = `p(Y=y,Z=0)`/`p(Y=y)`,
    `p(Z=1|Y=y)` = 1 - `p(Z=0|Y=y)` # == `p(Y=y,Z=1)`/`p(Y=y)`)
  )
  
  # M step
  `pi-hat-prev` = `p(Z=0)` # save the previous pi-hat estimate so we can graph our progress
  `p(Z=0)` = mean(Estep$`p(Z=0|Y=y)`) # here's the new pi-hat estimate
  
  Q = function(pi1)
  {
    sapply(pi1, 
      FUN = function(x)
      {
        with(Estep, sum((log(`p(Y=y|Z=0)`) + log(x))*`p(Z=0|Y=y)` +
            (log(`p(Y=y|Z=1)`) + log(1-x))*`p(Z=1|Y=y)`))
      })
  }
  
  # plot(loglik, xlim = c(0,1), xlab = "p(Z=0)", ylab = "log(likelihood)", lwd = 2)
  points(x = `pi-hat-prev`, y = loglik(`pi-hat-prev`), col = "red", pch = 16)
  plot(Q, add = TRUE, col = 'blue', lwd = 2)
  legend(x = "topleft", col = c('black', 'blue'), lty = 1, 
    lwd = 2,
    legend = c("log-likelihood", expression(Q(pi,hat(pi)))))
  points(x = `p(Z=0)`, y = loglik(`p(Z=0)`), pch = 16, col = 'orange')
  points(x = `p(Z=0)`, y = Q(`p(Z=0)`), col = "green", pch = 16)
  
  
  
  diff = `p(Z=0)` - `pi-hat-prev`
  new_results = tibble(
    Iteration = i,
    `p(Z=0)` = `p(Z=0)`,
    loglik = loglik(`p(Z=0)`),
    `diff(loglik)` = diff)
  
  progress = 
    bind_rows(progress, new_results)
  
  if(diff < tolerance) break;  
}

pander(progress)


```


