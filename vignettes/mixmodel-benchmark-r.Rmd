---
title: "mixmodel-benchmark-r"
output: 
  rmarkdown::html_vignette:
    code_folding: show
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{mixmodel-benchmark-r}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```

This R code defines an Expectation-Maximization (EM) algorithm to estimate the parameters of a mixture of two Gaussian distributions. The EM algorithm is an iterative method for finding the maximum likelihood estimates of parameters in models with latent (unobserved) variables. The Gaussian mixture model in this code assumes that the observed data, Y, is generated from either one of two Gaussian distributions with means mu[1] and mu[2] and a common standard deviation sigma. The probability of the data coming from the first Gaussian distribution is pi, while the probability of the data coming from the second Gaussian distribution is 1 - pi.

The gen_data function generates data according to this Gaussian mixture model, while the fit_model function performs the EM algorithm. The E_step function calculates the expected value of the log-likelihood given the current estimates of the parameters and the observed data, while the M_step updates the estimates of the parameters based on the expected log-likelihood. The algorithm continues to iterate until the difference in the log-likelihood between consecutive iterations is less than a specified tolerance (tolerance). The results of the algorithm are saved in a progress tibble that shows the values of the estimated parameter pi-hat and the log-likelihood for each iteration.

```{r setup}

library(dplyr)
library(pander)
```

# Generate data

```{r define gen data}

gen_data = function(
    n = 500000,
    mu = c(0,2),
    sigma = 1,
    pi = 0.8)
{
  
  tibble(
    Obs.ID = 1:n,
    Z = (runif(n) > pi) + 1,
    Y = rnorm(n = n, mean = mu[Z], sd = sigma)
  ) |>
    select(-Z) |>
    mutate(
      `p(Y=y|Z=1)` = dnorm(Y, mu[1], sd = sigma),
      `p(Y=y|Z=2)` = dnorm(Y, mu[2], sd = sigma),
    )
  
}

```

```{r}
set.seed(1)
data = gen_data(n = 500000, pi = 0.8)
data |> head(6) |> pander()
```

# Fit model

```{r}

fit_model = function(
    data,
    `p(Z=1)` = 0.5, # initial guess for `pi-hat`
    tolerance = 0.00001,
    max_iterations = 1000,
    verbose = FALSE
)
{
  
  # pre-allocate a table of results by iteration:
  progress = tibble(
      Iteration = 0:max_iterations,
      `p(Z=1)` = NA_real_,
      loglik =  NA_real_,
      diff_loglik = NA_real_
    )
  
  # initial E step, to perform needed calculations for initial likelihood:
  data = data |> E_step(`p(Z=1)` = `p(Z=1)`) 
  ll = loglik(data)
  progress[1, ] = list(0, `p(Z=1)`, ll, NA)
  
  for(i in 1:max_iterations)
  {
    # M step: re-estimate parameters
    `p(Z=1)` = data |> M_step()
    
    # E step: re-compute distribution of missing variables, using parameters
    data = data |> E_step(`p(Z=1)` = `p(Z=1)`)
    
    # Assess convergence
    
    ## save the previous log-likelihood so we can test for convergence
    ll_old = ll
    
    ## here's the new log-likelihood
    ll = loglik(data)
    
    ll_diff = ll - ll_old
    
    progress[i+1, ] = list(i, `p(Z=1)`, ll, ll_diff)
    
    if(verbose) print(progress[i+1, ])
    
    if(ll_diff < tolerance) break;
  }
  
  return(progress[1:(i+1), ])
}

```

### E step

```{r}

E_step = function(data, `p(Z=1)`)
{
  data |>
    mutate(
      `p(Y=y, Z=1)` = `p(Y=y|Z=1)` * `p(Z=1)`,
      `p(Y=y, Z=2)` = `p(Y=y|Z=2)` * (1 - `p(Z=1)`),
      `p(Y=y)`      = `p(Y=y, Z=1)` + `p(Y=y, Z=2)`,
      `p(Z=1|Y=y)`  = `p(Y=y, Z=1)` / `p(Y=y)`,
    )
}

```

### M step

```{r}

M_step = function(data)
{
  data |> pull(`p(Z=1|Y=y)`) |> mean()
}

```

### Log-likelihood

Compute the log-likelihood of the observed data given current parameter estimates:

```{r}

loglik = function(data)
{
  data |> pull(`p(Y=y)`) |> log() |> sum()
}

```

# Results

Finally, the `system.time()` function is used to measure the time it takes to run the `fit_model()` function:

```{r}
{results = fit_model(data, tolerance = 0.00001)} |> 
  system.time()

```

```{r}
print(results, n = Inf)
```

